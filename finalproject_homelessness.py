# -*- coding: utf-8 -*-
"""FinalProject_Homelessness.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JR54t_vZOyBQh7hjMn3lmk83zjgKF3VR
"""

# Commented out IPython magic to ensure Python compatibility.
import requests
from IPython.core.display import HTML
styles = requests.get(
    "https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/"
    "themes/static/css/cs109.css"
).text
HTML(styles)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import itertools

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import resample
from sklearn import tree
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import svm
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance

# pandas tricks for better display
pd.set_option('display.width', 1500)
pd.set_option('display.max_columns', 100)

# %matplotlib inline

"""<a id="intro"></a>

## Overview and data description
The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data 14 features for homes from various suburbs in Boston, Massachusetts.

### Data description
The following describes the dataset columns:
- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town.
- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per 10,000 dollars
- PTRATIO - pupil-teacher ratio by town
- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT - percent lower status of the population
- MEDV - Median value of owner-occupied homes in 1000's of dollars
"""

from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded['boston.csv']))

# Import data
# data = pd.read_csv('boston.csv')
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
data = data[column_names]

"""## Exploratory Data Analysis"""

# First 5 rows
data.head()

# Dimension of the data
np.shape(data)

# Data info
data.info()

# Designate categorical variables

cat_type = pd.CategoricalDtype(ordered=False) 
data['CHAS'] = data['CHAS'].astype(cat_type)

cat_type_ord = pd.CategoricalDtype(ordered=True) 
data['RAD'] = data['RAD'].astype(cat_type)

# Number of missing values
data.isnull().sum() # no missing values

# Summary of non-categorical data
data.describe()

# Summary of categorical data
print(data['CHAS'].unique()) # 2 categories
print(data['RAD'].unique()) # 9 categories

# Plot histograms of variables

plt.figure(figsize=(16, 8)) 

for i in column_names:
    plt.subplot(3, 5, column_names.index(i)+1)
    plt.hist(data[i])
    plt.xlabel(i)
    plt.ylabel("Frequency")
    plt.title(i)
    plt.tight_layout()

# Explore correlations between variables

plt.figure(figsize=(20, 10))
sns.heatmap(data.corr().abs(), vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True, annot=True)

# 'RM', 'PTRATIO', and 'LSTAT' are all correlated > .4 with MEDV, indicating they may be good predictors

# Plot highly correlated variables with 'MEDV'

highly_corr_vars = ['RM', 'PTRATIO', 'LSTAT']

plt.figure(figsize=(9, 3)) 

for i in highly_corr_vars:
    plt.subplot(1, 3, highly_corr_vars.index(i)+1)
    plt.scatter(data[i], data['MEDV'], alpha = 0.4)
    plt.xlabel(i)
    plt.ylabel("MEDV")
    plt.title(i)
    plt.tight_layout()

"""## Baseline Model - Linear regression with all predictors

After exploring the data through an EDA and completing the necessary data cleaning and preparation, we began our analysis by splitting the full dataset into an 80% / 20% training and test split. 

For our baseline model, we ran a simple linear regression using ‘LinearRegression’ from sklearn on our training sample. We designated our response variable as MEDV (median home value) and included all predictors included in the model. 

We then predicted on our test sample and calculated the mean squared error (MSE) for our training sample, MSE for our test sample, and an R-squared score, to evaluate the performance of our baseline model.  
"""

## Split into train and test

# Designate predictors and response
predictors = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
X = data[predictors]
y = data['MEDV']

# Normalize
X_norm = preprocessing.normalize(X)

# Split into train and test
X_train_std, X_test_std, y_train, y_test = train_test_split(X_norm, y, random_state=42, test_size=0.2)

# Standardize
#sc = StandardScaler()
#X_train_std = sc.fit_transform(X_train)
#X_test_std = sc.transform(X_test)

## Conduct a linear regression to predict 'MEDV' from all predictors

# Initialize a Linear Regression model
lreg = LinearRegression()

# Fit the linear model on the train data
lreg.fit(X_train_std, y_train)

# Predict on the test data
y_test_pred = lreg.predict(X_test_std)
y_train_pred = lreg.predict(X_train_std)

# Use the mean_squared_error and r2_score functions to compute the test mse and r-squared
mse_test = mean_squared_error(y_test, y_test_pred)
mse_train = mean_squared_error(y_train, y_train_pred)
r2 = r2_score(y_test, y_test_pred)

# Print the MSE and r^2 values
print("The regression train MSE is", mse_train)
print("The regression test MSE is", mse_test)
print("The r-squared is", r2)

## Extract and plot coefficients

# Create a dictionary of the coefficients with the predictors as keys
lreg_coef = dict(zip(X.columns, np.transpose(lreg.coef_)))

# Linear regression coefficients
lreg_x = list(lreg_coef.keys())
lreg_y = list(lreg_coef.values())

# Print the coefficients
print("The regression coefficients are:", lreg_coef)

# Plot the coefficients
plt.rcdefaults()
plt.barh(lreg_x, lreg_y, 1.0, align='edge', color="cornflowerblue", label = "Linear Regression")
plt.grid(linewidth = 0.3)
plt.xlabel("Coefficient")
plt.ylabel("Predictors")
plt.title('')
plt.legend(loc = 'best')
plt.show()

"""From this model, we learned that some predictors were more strongly related to MEDV. NOX, or the nitric oxides concentration, was negatively related to MEDV such that more nitric oxides relates to a lower median home value. Conversely, CHAS and RM were positively related to MEDV, such that homes on the river and homes with more rooms were related to a higher median home value.

## Regularization - Lasso Regression

Given our goal to select a subset of variables that are important in predicting home values, we chose to conduct a lasso regression next. This regularization method uses shrinkage to push some coefficients towards zero thereby eliminating them from the model, increasing interpretability and prediction accuracy. We fit a well-tuned lasso regression using ‘Lasso’ from sklearn. We selected the hyperparameter (alpha) that resulted in the smallest test error, and again calculated test MSE, train MSE, and R-squared values to assess model performance.
"""

alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]
lasso_test_mses = []

for alpha in alphas:
    
    # Create a Lasso Regression model with alpha
    lasso_reg = Lasso(alpha = alpha)

    # Fit the model on the train data
    lasso_reg.fit(X_train_std, y_train)

    # Predict on the test data using the trained model
    y_test_pred = lasso_reg.predict(X_test_std)
    
    # Calculate the test MSE
    mse_lasso = mean_squared_error(y_test, y_test_pred)

    # Store test MSE to list
    lasso_test_mses.append(mse_lasso)

# Find the best hyperparameter (alpha), i.e. the one that gives the smallest test error
best_parameter = alphas[lasso_test_mses.index(min(lasso_test_mses))]
print('The best alpha value for our lasso regression is', best_parameter)
  
# Run with best alpha parameter
lasso_reg = Lasso(alpha = best_parameter)
lasso_reg.fit(X_train_std, y_train)
y_test_pred = lasso_reg.predict(X_test_std)
y_train_pred = lasso_reg.predict(X_train_std)
lasso_reg_coefs = lasso_reg.coef_
lasso_reg_intercept = lasso_reg.intercept_

# Print MSE and r-squared values
mse_test_lasso = mean_squared_error(y_test, y_test_pred)
mse_train_lasso = mean_squared_error(y_train, y_train_pred)
r2_lasso = r2_score(y_test, y_test_pred)
print("The lasso regression test MSE is", mse_lasso)
print("The lasso regression train MSE is", mse_train_lasso)
print("The lasso regression r-squared is", r2_lasso)
    
# Make a dictionary of the predictors and the coefficients associated with them
lasso_coef = dict(zip(X.columns, np.transpose(lasso_reg.coef_))) 

# Get the Lasso coefficients for plotting
lasso_x = list(lasso_coef.keys())
lasso_y = list(lasso_coef.values())

# Print the coefficients
print("The lasso regression coefficients are:", lasso_coef)

# Plot the coefficients, along with the previous linear regression coefficients
plt.rcdefaults()
plt.barh(lreg_x, lreg_y, 1.0, align='edge', color="cornflowerblue", label = "Linear Regression")
plt.barh(lasso_x, lasso_y, 0.75, align='edge', color="yellowgreen", label = "Lasso Regression")
plt.grid(linewidth = 0.3)
plt.xlabel("Coefficient")
plt.ylabel("Predictors")
plt.legend(loc = 'best')
plt.show()

# Important predictors
bool_important_coefs = (lasso_reg_coefs != 0)
predictors_important = list(
    np.array(predictors)[bool_important_coefs]
)

print(
    "The following predictors were deemed important by "
    "the lasso regression model (i.e. coef != 0):\n\n\t{}\n\n\n"
    "While, the remaining were deemed unimportant (i.e. "
    "coef == 0):\n\n\t{}"
    .format(
        predictors_important,
        np.array(predictors)[~bool_important_coefs],
    )
)

"""From this plot, we see that some predictors that were deemed ‘unimportant’ by the model (i.e., predictors with coefficients that were pushed to zero), and some were designated as ‘important’ (i.e., predictors with coefficients > 0).

## Regularization - Lasso Regression with Interaction Terms

To further explore how our predictors relate to our outcome variable, we examined whether the interaction between each of our predictors relates to MEDV. Interactions are important to explore and include in a model when the effect of one variable changes depending on the value of another variable. Lasso regression is a well-suited method to explore interactions, as we can calculate the interactions between all pairs of variables, and the model will eliminate interaction terms that do not predict our outcome variable, resulting in a model that is still interpretable. 

We selected the ten predictors that were deemed ‘important’ by our lasso model and created interaction terms between each pair of variables. We conducted a well-tuned lasso regression with the important predictors and their interactions on the training sample, and evaluated the model on the test sample.
"""

# We'll now fit a well-tuned Lasso regression model with all important predictors from our lasso model
# above and all the unique 2-way interactions between them

# Make arrays into dataframes
X_train_std = pd.DataFrame(X_train_std, columns = predictors)
X_test_std = pd.DataFrame(X_test_std, columns = predictors)

predictors_important = ['CRIM', 'ZN', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

# Use `itertools.combinations` function to generate our interaction term combinations
combinations = list(
    itertools.combinations(predictors_important, 2)
)

interaction_colnames = []

for a, b in combinations:
    colname = '{} {}'.format(a, b)
    X_train_std[colname] = X_train_std[a].astype(float) * X_train_std[b].astype(float)
    X_test_std[colname] = X_test_std[a].astype(float) * X_test_std[b].astype(float)
    interaction_colnames.append(colname)

# Check new interaction columns were added
X_train_std.columns

# Run new tuned lasso regression model with interaction terms

alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]
lasso_test_mses_int = []

for alpha in alphas:
    
    # Create a Lasso Regression model with alpha
    lasso_reg_int = Lasso(alpha = alpha, max_iter = 100000) #(number of iterations was increased to solve convergence wa

    # Fit the model on the train data
    lasso_reg_int.fit(X_train_std, y_train)

    # Predict on the test data using the trained model
    y_test_pred = lasso_reg_int.predict(X_test_std)
    
    # Calculate the test MSE
    mse_lasso = mean_squared_error(y_test, y_test_pred)

    # Store test MSE to list
    lasso_test_mses_int.append(mse_lasso)

# Find the best hyperparameter (alpha), i.e. the one that gives the smallest test error
best_parameter = alphas[lasso_test_mses.index(min(lasso_test_mses))]
print('The best alpha value for our lasso regression with interaction terms is', best_parameter)
  
# Run with best alpha parameter
lasso_reg_int = Lasso(alpha = best_parameter, max_iter = 100000) #(number of iterations was increased to solve convergence warning)
lasso_reg_int.fit(X_train_std, y_train)
y_test_pred = lasso_reg_int.predict(X_test_std)
y_train_pred = lasso_reg_int.predict(X_train_std)
lasso_reg_int_coefs = lasso_reg_int.coef_
lasso_reg_int_intercept = lasso_reg_int.intercept_

# Print MSE and r-squared values
mse_test_lasso_int = mean_squared_error(y_test, y_test_pred)
mse_train_lasso_int = mean_squared_error(y_train, y_train_pred)

r2_lasso_int = r2_score(y_test, y_test_pred)
print("The lasso regression with interaction terms test MSE is", mse_test_lasso_int)
print("The lasso regression with interaction terms train MSE is", mse_train_lasso_int)
print("The lasso regression with interaction terms r-squared is", r2_lasso_int)
    
# Make a dictionary of the predictors and the coefficients associated with them
lasso_coef_int = dict(zip(X_train_std.columns, np.transpose(lasso_reg_int.coef_))) 

# Get the Lasso coefficients for plotting
lasso_x_int = list(lasso_coef_int.keys())
lasso_y_int = list(lasso_coef_int.values())

# Print the coefficients
print(
    "The intercept and coefficients for the lasso regression with interaction terms model are:"
    "\n\n\t{:<20}{:.4f}".format(
        "intercept", lasso_reg_int_intercept
    )
)

for predictor, coef in zip(list(X_train_std.columns), lasso_reg_int_coefs):
    print("\t{:<20}{:.4f}".format(predictor, coef))

# Important predictors
bool_important_coefs = (lasso_reg_int_coefs != 0)
predictors_important = list(
    np.array(X_train_std.columns)[bool_important_coefs]
)

print(
    "The following predictors were deemed important by "
    "the lasso regression model with interactions (i.e. coef != 0):\n\n\t{}\n\n\n"
    "While, the remaining were deemed unimportant (i.e. "
    "coef == 0):\n\n\t{}"
    .format(
        predictors_important,
        np.array(X_train_std.columns)[~bool_important_coefs],
    )
)

"""As expected, many of the coefficients of the predictors were shrunk to zero, and several predictors, listed above, remained 'important'. 

Interpreting these coefficients, we see that lower crime rates, more rooms, closer distance to Boston employment center, more accessibility to highways, a higher tax rate, and a lower pupil-teacher ratio predict higher median home values in Boston. From the interaction terms, we see that several variables interact with B, which refers to a formula that includes the number of black individuals. This formula makes this variable hard to interpret, however, as the transformation assumes parabolic relationship between the proportion of black individuals and housing prices. If we were to analyze this datset in a future analysis, we would need to obtain the raw data to look at the raw proportion of black individuals to fully parse these interactions.

## Comparison of Linear Regression, Lasso Regression, and Lasso + Interaction
"""

d = {'Model': ["Linear Regression", "Lasso Regression", "Lasso Regression with Interaction Terms"], 
     'Training MSE': [mse_train, mse_train_lasso, mse_train_lasso_int], 
     'Test MSE': [mse_test, mse_test_lasso, mse_test_lasso_int],
     'R-Squared': [r2, r2_lasso, r2_lasso_int]}
linear_results_df = pd.DataFrame(data=d)
linear_results_df

"""Our linear regression performed best on the training dataset, but our lasso regression without interaction terms had the lowest test MSE and the highest R-squared. Thus, between these three models, the lasso regression performed best.

## PCA Model - Controlling for collinearity
The correlation matrix above shows highly correlated predictors (>0.7). Is it possible to use a PCA transformation to eliminate possible effects of multicollinearity?
"""

# Split the data into train and test sets
predictors = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
X = data[predictors]
y = data['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

# Standardize X_train and X_test
sc = StandardScaler().fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

k_list = np.arange(1,len(predictors)+1)
validation_mse_list = []

# Apply the PCA transformation
for k in k_list:
    pca = PCA(n_components=k)
    X_train_k = pca.fit_transform(X_train_std)
    mse_score = cross_validate(LinearRegression(), X_train_k, y_train, cv=10,\
    scoring="neg_mean_squared_error")

    validation_mse_list.append(-1*mse_score['test_score'].mean())

# Plot MSE vs PCs for each value of k
plt.figure(figsize=(12, 9))
plt.plot(k_list, validation_mse_list, marker='o', ls='--')
plt.xlabel('Number of PCs retained', fontsize=15)
plt.ylabel('Mean Validation MSE', fontsize=15)
plt.title('Mean Validation MSE vs Number of PCs retained', fontsize=18)
plt.show()

best_k = k_list[np.argmin(validation_mse_list)]
print(f"The best k is {best_k}.")

# Recalculate PCA with best k
pca = PCA(n_components=best_k)
X_train_k = pca.fit_transform(X_train_std)
X_test_k = pca.transform(X_test_std)
pcreg = LinearRegression().fit(X_train_k, y_train)
y_pred = pcreg.predict(X_test_k)
y_pred_t = pcreg.predict(X_train_k)

pcr_MSE_train = mean_squared_error(y_train, y_pred_t)
print(f"MSE train for PCA model is {pcr_MSE_train:.4f}")

pcr_MSE_test = mean_squared_error(y_test, y_pred)
print(f"MSE test for PCA model is {pcr_MSE_test:.4f}")

pcr_r2_score = r2_score(y_test, y_pred)
print(f"R^2 score for PCA model is {pcr_r2_score:.4f}")

"""**It appears the answer to the question posed above is no**--the model needs all predictors to match performance of the baseline linear model.

## Interaction PCA Model - Controlling for collinearity
The PCA transformation only using the original 13 predictors did not improve the baseline model. The question then becomes: is it possible to add interaction terms and then use a PCA transformation to eliminate possible effects of multicollinearity?
"""

# Create interaction terms for train and test
interaction = PolynomialFeatures(2, interaction_only=True, include_bias=False)
X_train_int = interaction.fit_transform(X_train_std)
X_test_int = interaction.fit_transform(X_test_std)

k_list = np.arange(1,X_train_int.shape[1]+1)
validation_mse_list = []

# Apply the PCA transformation
for k in k_list:
    pca = PCA(n_components=k)
    X_train_k = pca.fit_transform(X_train_int)
    mse_score = cross_validate(LinearRegression(), X_train_k, y_train, cv=10,\
    scoring="neg_mean_squared_error")

    validation_mse_list.append(-1*mse_score['test_score'].mean())

# Plot MSE vs PCs for each value of k
plt.figure(figsize=(12, 9))
plt.plot(k_list, validation_mse_list, marker='o', ls='--')
plt.xlabel('Number of PCs retained', fontsize=15)
plt.ylabel('Mean Validation MSE', fontsize=15)
plt.title('Mean Validation MSE vs Number of PCs retained', fontsize=18)
plt.show()

best_k_int = k_list[np.argmin(validation_mse_list)]
print(f"The best k with interaction terms is {best_k_int}.")

# Recalculate PCA with best k
pca_int = PCA(n_components=best_k_int)
X_train_k_int = pca_int.fit_transform(X_train_int)
X_test_k_int = pca_int.transform(X_test_int)
pcreg_int = LinearRegression().fit(X_train_k_int, y_train)
y_pred_int = pcreg_int.predict(X_test_k_int)
y_pred_t_int = pcreg_int.predict(X_train_k_int)

pcr_r2_score_int = r2_score(y_test, y_pred_int)
print(f"R^2 score for interaction PCA model is {pcr_r2_score_int:.4f}")

pcr_MSE_train_int = mean_squared_error(y_train, y_pred_t_int)
print(f"MSE train for interaction PCA model is {pcr_MSE_train_int:.4f}")

pcr_MSE_test_int = mean_squared_error(y_test, y_pred_int)
print(f"MSE test for interaction PCA model is {pcr_MSE_test_int:.4f}")

d = {'Model': ["PCA Linear Regression", "PCA Linear Regression with Interaction Terms"], 
     'Training MSE': [pcr_MSE_train, pcr_MSE_train_int], 
     'Test MSE': [pcr_MSE_test, pcr_MSE_test_int],
     'R-Squared': [pcr_r2_score, pcr_r2_score_int]}
pcr_results_df = pd.DataFrame(data=d)
pcr_results_df

"""**The answer to the second question is yes**--the model can be significantly improved by including just interaction terms and then performing a principle component analysis. Looking at the variance plots below, nearly all of the model variance can be explained by 39 principle components when including interaction terms.


"""

fig, ax = plt.subplots(nrows=2, figsize=(20,15))
ax1, ax2 = ax.ravel()

# Plot the variance explained by each component
ratio = pca_int.explained_variance_ratio_
xticks = np.arange(1,pca_int.explained_variance_ratio_.shape[0]+1)
ax1.bar(range(len(ratio)), ratio, alpha=0.8)
ax1.set_title('Variance Explained by Each Component', fontsize=20)
ax1.set_xticks(range(len(ratio)))
ax1.set_xticklabels(xticks)
ax1.set_ylabel('Percent of Model Variance', fontsize=15)
ax1.set_xlabel('Principle Component Number', fontsize=15)
ax1.grid()

# Plot the cumulative sum of variance explained by each compoment
ax2.plot(np.cumsum(ratio), 'o-')
ax2.set_title('Cumulative Sum of Variance After Each Component', fontsize=20)
ax2.set_ylim(0,1.1)
ax2.set_xticks(range(len(ratio)))
ax2.set_xticklabels(xticks)
ax2.set_ylabel('Cumulative Sum', fontsize=15)
ax2.set_xlabel('Principle Component Number', fontsize=15)
ax2.grid();

"""One important note, however:

*****INTERACTION PCA MODEL IS NOT EASY TO INTERPRET LIKE THE LASSO REGRESSION WITH INTERACTION TERMS*****

The Lasso model with interaction terms is easier to understand (better interpretability). Interpretability is a major goal of this exercise. Teasing out the predictors that make up our 39 principle components would be tedious and wouldn't bring us closer to understanding which factors affect median home values the most. This model is an interesting experiment, but ultimately there are better methods to utilize that are easier to understand.

## Decision Tree Regressor

<div class='exercise-r'>

We first determine the best depth for our decision tree regressor using cross-validation. To do this, we do the following. For each tree depth from 1 to 20 (inclusive):

- Fit a decision tree to the entire **training** set.

- Evaluate on the entire **training** set.

- Perform 5-fold cross-validation with the entire **training** set, while storing the mean validation mean squared error and the validation standard deviation in variables named `cvmeans` and `cvstds`, respectively.

</div>
"""

max_depth = 20

training_error = []

# for storing mean validation score and validation standard deviation scores
cvmeans, cvstds = [], []

# for each tree depth from 1 to 20 inclusive
for depth in range(1, max_depth + 1):
    
    # Initialize a Decision Tree regressor
    dtree = DecisionTreeRegressor(max_depth=depth, random_state=109)

    # Fit a decision tree to the entire training set.
    dtree.fit(X_train_std, y_train)
    
    # Perform cross-validation on the entire data with 10 folds and get the mse_scores
    mse_score = cross_validate(dtree, X_train_std, y_train, cv=10, scoring="neg_mean_squared_error", 
                               return_train_score=True)
    
    # Compute the standard deviation of the validation MSE and store in list
    cvstds.append(np.std(mse_score['test_score']))
    
    # Compute the mean of the training error and store in list 
    training_error.append(-1*np.mean(mse_score['train_score']))
    
    # Compute the mean of the cross validation error and store in list 
    cvmeans.append(-1*np.mean(mse_score['test_score']))

# Get the numbers for the +/- 0.5 standard deviation bounds
half_stds = [0.5 * std for std in cvstds]

# Create a plot of the train and test accuracies to get the best tree depth
fig, axes = plt.subplots(1,2, figsize=(20, 10))
axes = axes.flatten()

for idx, ax in enumerate(axes):
    
    # Plot the training accuracy with labels
    ax.plot(range(1, max_depth + 1), training_error, label = 'Training MSE', linewidth=3, 
             color='#FF7E79', alpha=0.4)

    # Plot the test accuracy with labels
    ax.plot(range(1, max_depth + 1), cvmeans, label = 'Cross Validation MSE', linewidth=3, 
             color="#007D66", alpha=0.4)

    # Plot the +/- 2 standard deviation bounds
    ax.fill_between(range(1, max_depth + 1), np.array(cvmeans) - (half_stds), 
                     np.array(cvmeans) + (half_stds), label = 'Standard Deviation Bounds',alpha=0.4)

    # Set the labels
    ax.set_xlabel('Depth of Tree', fontsize=12)
    ax.set_xticks(range(1, 21))
    ax.set_ylabel('Mean Squared Error', fontsize=12)
    ax.set_title("Mean Squared Error as a function of decision tree depth")
    ax.legend(frameon = True, loc = 'upper left')
    ax.grid(":", alpha=0.4)
    if idx == 1:
        ax.legend(frameon = True, loc = 'upper right')
        ax.set_ylim(10, 50)
    idx += 1

# Get the lowest validation MSE value
min_cv_mse = min(cvmeans)
# Get the depth corresponding to the lowest MSE value
best_depth = cvmeans.index(min_cv_mse) + 1

print(f'The best depth is: {best_depth}')

best_depth = 13

best_dtr = DecisionTreeRegressor(max_depth=best_depth, random_state=109)

best_dtr.fit(X_train_std, y_train)

y_train_pred = best_dtr.predict(X_train_std)

y_test_pred = best_dtr.predict(X_test_std)

best_cv_tree_train_mse = mean_squared_error(y_train_pred, y_train)
best_cv_tree_test_mse = mean_squared_error(y_test_pred, y_test)

best_r_squared_dtr = best_dtr.score(X_test_std, y_test)

print(f'The best train MSE is: {best_cv_tree_train_mse:.4f}')
print(f'The best test MSE is: {best_cv_tree_test_mse:.4f}')
print(f'The r^2 of the best model is: {best_r_squared_dtr:.4f}')

"""# Bagging with Decision Tree Regressors"""

tree_depth = 20

dtree_overfitting = DecisionTreeRegressor(max_depth = tree_depth, random_state=109)

dtree_overfitting.fit(X_train_std, y_train)

y_pred_train = dtree_overfitting.predict(X_train_std)

y_pred_test = dtree_overfitting.predict(X_test_std)

dtree_over_train_mse = mean_squared_error(y_pred_train, y_train)

dtree_over_test_mse = mean_squared_error(y_pred_test, y_test)

r_squared_dtr_over = dtree_overfitting.score(X_test_std, y_test)

print(f'The train MSE of a decision tree of depth {tree_depth} is: {dtree_over_train_mse:.4f}')
print(f'The test MSE of a decision tree of depth {tree_depth} is: {dtree_over_test_mse:.4f}')
print(f'The r^2 of a decision tree of depth {tree_depth} is: {r_squared_dtr_over:.4f}')

"""<div class='exercise-r'>

Here we will use the `tree_depth` chosen above to generate 55 boostrapped sets of decision tree predictions for both the training and test data. To accomplish this:

- First, we create a `bagger` function to generate 55 boostrapped sets of decision tree predictions for both the training and test data.
- Then, make a single call to the `bagger` function to return our bootstrapped results.
- The function stores the returned results as: 
  1. `bagging_train_df`: a dataframe containing the training data predictions
  2. `bagging_test_df`: a dataframe containing the test data predictions
  3. `bagging_models_list`: a list containing the 55 fitted model objects (i.e. fitted estimators)

</div>
"""

def bagger(
    n_trees: int,
    tree_depth: int,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    random_seed: int = 0,
) -> (pd.DataFrame, pd.DataFrame, list):
    """
    :param n_trees: int, number of bootstrapped decision trees
    :param tree_depth: int, maximum tree depth
    :param X_train: np.ndarray, training X observations
    :param y_train: np.ndarray, training y observations
    :param X_test: np.ndarray, test X observations
    :param random_seed: int, random seed used to set np.random.seed
                        to ensure replicable results (default=0)
    
    :returns: (pd.DataFrame, pd.DataFrame, list), tuple containing 3
              objects, (1) bagging_train_df dataframe, (2) bagging_test_df
              and (3) bagging_models_list
              containing every trained DecisionTreeRegressor model
              object (i.e. estimator), one estimator for each bootstrap
    """
    bagging_models_list = []
    # set random seed
    np.random.seed = random_seed
    
    # instantiate arrays and list for storing results
    bagging_train = np.zeros((X_train.shape[0], n_trees)).astype(int)
    bagging_test = np.zeros((X_test.shape[0], n_trees)).astype(int)

    # perform n bootstraps
    for i in range(n_trees):
        # generate bootstrapped model
        bootstrapped_X, bootstrapped_y = resample(X_train, y_train)
        
        model = DecisionTreeRegressor(max_depth=tree_depth, random_state=109)
        
        fitted_model = model.fit(bootstrapped_X, bootstrapped_y)
        
        bagging_models_list.append(fitted_model)

        # predict on full training and test sets and store
        # results to arrays
        bagging_train[:,i] = fitted_model.predict(X_train)
        bagging_test[:,i] = fitted_model.predict(X_test)
    
    # convert arrays to pandas dataframes
    bagging_train_df = pd.DataFrame(
        bagging_train[:, :],
        columns=[f"bootstrap model {x}" for x in range(n_trees)],
    )
    bagging_test_df = pd.DataFrame(
        bagging_test[:, :],
        columns=[f"bootstrap model {x}" for x in range(n_trees)],
    )
    
    return bagging_train_df, bagging_test_df, bagging_models_list

assert tree_depth == 20
# specify number of required bootrapped trees
n_trees = 55 

# generate predictions using bagger function
bagging_train_df, bagging_test_df, bagging_models_list = bagger(
    n_trees, tree_depth, X_train_std, y_train, X_test_std, random_seed=0
)

# display resulting dataframe heads
display(bagging_train_df.head())
display(bagging_test_df.head())

bag_y_pred_train = np.mean(bagging_train_df.iloc[:, :], axis=1)

bag_y_pred_test = np.mean(bagging_test_df.iloc[:, :], axis=1)

bagging_mse_train = mean_squared_error(bag_y_pred_train, y_train)

bagging_mse_test = mean_squared_error(bag_y_pred_test, y_test)

bagging_r2 = r2_score(bag_y_pred_test, y_test)

print(f'The MSE of the bagging model on the train set is {bagging_mse_train:.4f}')
print(f'The MSE of the bagging model on the test set is {bagging_mse_test:.4f}')
print(f'The r^2 of the bagging model on the test set is {bagging_r2:.4f}')

"""## Random Forest Regressor

<div class='exercise-r'>

We fit a `RandomForestRegressor` to the scaled `X_train_std` data, and set the maximum number of features to use when looking for the best split to be the square root of the total number of features. We then evaluate and report this regressor's mean squared error on the training and test sets. Then, we assign those error values to the variables `random_forest_train_mse` and `random_forest_test_mse`.

</div>
"""

# your code here
assert tree_depth == 20
# Define a Random Forest regressor
random_forest = RandomForestRegressor(max_depth=tree_depth, 
                                      n_estimators=55, 
                                      max_features="sqrt", 
                                      random_state=109)

# Fit the model on the training set
random_forest.fit(X_train_std, y_train)

rf_y_pred_train = random_forest.predict(X_train_std)

rf_y_pred_test = random_forest.predict(X_test_std)

random_forest_train_mse = mean_squared_error(rf_y_pred_train, y_train)

random_forest_test_mse = mean_squared_error(rf_y_pred_test, y_test)

random_forest_r_squared = random_forest.score(X_test_std, y_test)

print(f'The train MSE for the random forest model is: {random_forest_train_mse:.4f}')

print(f'The test MSE for the random forest model is: {random_forest_test_mse:.4f}')

print(f'The r^2 for the random forest model is: {random_forest_r_squared:.4f}')

"""Among all of the decision trees we fit in the bagging process (i.e. each of the fitted model objects stored in bagging_models_list), we find how many times is each feature used as the top/first node. We do the same for each tree in the random forest we just fit. We assign the answers to these questions to two pandas Series called top_predictors_bagging and top_predictors_rf, and display them.

A decision tree's top feature is stored as .tree_.feature[0]. A random forest object stores its decision trees in its .estimators_ attribute.
"""

# extract top predictors from fitted random forest trees
top_predictors_rf = [
    data.columns[model.tree_.feature[0]]
    for model in random_forest.estimators_ 
]
# convert results to a pandas series
top_predictors_rf_df = pd.DataFrame(
    pd.Series(top_predictors_rf).value_counts(),
    columns=["Count"],
)
# display results
print(
    f'Random forest "top predictor" frequencies for '
    f"each of the {n_trees} ensemble trees:"
)
display(top_predictors_rf_df)

# extract top predictors from fitted bagger bootstrapped trees
top_predictors_bagging = [
    data.columns[model.tree_.feature[0]]
    for model in bagging_models_list
]
# convert results to a pandas series
top_predictors_bagging_df = pd.DataFrame(
    pd.Series(top_predictors_bagging).value_counts(),
    columns=["Count"],
)
# display results
print(
    f'\nBagging "top predictor" frequencies for '
    f"each of the {n_trees} ensemble trees:"
)
display(top_predictors_bagging_df)

"""# Gradient Boosting Regressor"""

gbr = GradientBoostingRegressor(n_estimators = 800, 
                                max_depth = 3, 
                                learning_rate = 0.08)
gbr.fit(X_train_std, y_train)

y_pred_train = gbr.predict(X_train_std)
y_pred_test = gbr.predict(X_test_std)

gbr_mse_train = mean_squared_error(y_pred_train, y_train)
gbr_mse_test = mean_squared_error(y_pred_test, y_test)
gbr_r2 = gbr.score(X_test_std, y_test)

print(f'The train MSE for the Gradient Boosted Machine is: {gbr_mse_train:.4f}')
print(f'The test MSE for the Gradient Boosted Machine is: {gbr_mse_test:.4f}')
print(f'The r^2 for the Gradient Boosted Machine is: {gbr_r2:.4f}')

fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,8))
i = 0

lr_test_score = []
lr_train_score = []
lr_rsquared = []

for l in np.linspace(0.000001, 0.4, 6):
    gbr_lr = GradientBoostingRegressor(n_estimators = 800, 
                                max_depth = 3, 
                                min_samples_split = 5, 
                                learning_rate = l, 
                                loss= 'ls')
    gbr_lr.fit(X_train_std, y_train)

    y_pred_train = gbr_lr.predict(X_train_std)
    y_pred_test = gbr_lr.predict(X_test_std)

    gbr_mse_train_lr = mean_squared_error(y_pred_train, y_train)
    gbr_mse_test_lr = mean_squared_error(y_pred_test, y_test)
    gbr_r2_lr = gbr_lr.score(X_test_std, y_test)

    lr_train_score.append(gbr_mse_train_lr)
    lr_test_score.append(gbr_mse_test_lr)
    lr_rsquared.append(gbr_r2_lr)
    
    #print(f'The train MSE for the Gradient Boosted Machine is: {gbr_mse_train:.4f}')
    #print(f'The test MSE for the Gradient Boosted Machine is: {gbr_mse_test:.4f}')
    #print(f'The r^2 for the Gradient Boosted Machine is: {gbr_r2:.4f}')
    
    train_scores = gbr.train_score_
    test_scores = np.zeros((800,), dtype=np.float64)
    for k, y_pred in enumerate(gbr.staged_predict(X_test_std)):
        test_scores[k] = gbr.loss_(y_test, y_pred)
    
axs.plot(np.linspace(0.000001, 0.4, 6), lr_train_score, color='blue', label = 'Training Score')
axs.plot(np.linspace(0.000001, 0.4, 6), lr_test_score, color='red', label = 'Test Score')

axs.set_xlabel('Learning Rate')
axs.set_ylabel('MSE')
axs.set_title('MSE vs Learning Rate')
axs.legend()
axs.grid(False)
i += 1

# your code here
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20,20))
axs = axs.ravel()

i = 0
for j in [1,3,15,20]:
    gbr_d = GradientBoostingRegressor(n_estimators = 800, 
                                max_depth = j, 
                                min_samples_split = 5, 
                                learning_rate = 0.08, 
                                loss= 'ls', random_state = 42)
    gbr_d.fit(X_train_std, y_train)

    y_pred_train_d = gbr_d.predict(X_train_std)
    y_pred_test_d = gbr_d.predict(X_test_std)

    gbr_mse_traing_d = mean_squared_error(y_pred_train_d, y_train)
    gbr_mse_test_d = mean_squared_error(y_pred_test_d, y_test)
    gbr_r2_d = gbr_d.score(X_test_std, y_test)

    #print(f'The train MSE for the Gradient Boosted Machine is: {gbr_mse_train:.4f}')
    #print(f'The test MSE for the Gradient Boosted Machine is: {gbr_mse_test:.4f}')
    #print(f'The r^2 for the Gradient Boosted Machine is: {gbr_r2:.4f}')
    
    train_scores = gbr_d.train_score_
    test_scores = np.zeros((800,), dtype=np.float64)
    for k, y_pred in enumerate(gbr_d.staged_predict(X_test_std)):
        test_scores[k] = gbr_d.loss_(y_test, y_pred)
    
    axs[i].scatter(range(1,801), train_scores, alpha = 0.2, color='blue', label = 'Training Score')
    axs[i].scatter(range(1,801), test_scores, alpha = 0.2, color='red', label = 'Test Score')

    axs[i].set_xlabel('Iterations')
    axs[i].set_ylabel('Loss')
    axs[i].set_title('MSE for Each Boosting Iteration for Depth = '+str(j))
    axs[i].legend()
    axs[i].grid(False)
    i += 1

# Get Feature importance data using feature_importances_ attribute

feature_importance = gbr.feature_importances_
print(feature_importance)
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5

fig = plt.figure(figsize=(8, 8))
plt.barh(pos, feature_importance[sorted_idx], align='center', color = 'cornflowerblue')
plt.yticks(pos, np.array(X.columns)[sorted_idx])
plt.title('Feature Importance for Gradiant Boosted Machine')
result = permutation_importance(gbr, X_test_std, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)

sorted_idx = result.importances_mean.argsort()
fig.tight_layout()
plt.show()

gbr = GradientBoostingRegressor(n_estimators = 800, 
                                max_depth = 3, 
                                learning_rate = 0.08)
gbr.fit(X_train_std, y_train)

y_pred_train = gbr.predict(X_train_std)
y_pred_test = gbr.predict(X_test_std)

gbr_mse_train = mean_squared_error(y_pred_train, y_train)
gbr_mse_test = mean_squared_error(y_pred_test, y_test)
gbr_r2 = gbr.score(X_test_std, y_test)

test_score = np.zeros((800,), dtype=np.float64)
for i, y_pred in enumerate(gbr.staged_predict(X_test_std)):
    test_score[i] = gbr.loss_(y_test, y_pred)

fig = plt.figure(figsize=(8, 8))
plt.subplot(1, 1, 1)
plt.plot(np.arange(800) + 1, gbr.train_score_, 'b-',
         label='Training Set Loss')
plt.plot(np.arange(800) + 1, test_score, 'r-',
         label='Test Set Loss')

plt.xlabel('Boosting Iterations')
plt.ylabel('Loss')
plt.title('Model Loss vs Iterations in Boosting')

fig.tight_layout()
plt.show()

data = [{'Model': 'single depth-12  tree chosen by CV', 
         'Training MSE': best_cv_tree_train_mse, 
         'Test MSE': best_cv_tree_test_mse,
         'R-Squared': best_r_squared_dtr}, 
        {'Model': 'single overfit depth-20  tree', 
         'Training MSE': dtree_over_train_mse, 
         'Test MSE': dtree_over_test_mse,
         'R-Squared': r_squared_dtr_over}, 
        {'Model': 'bagging 55 depth-20  trees', 
         'Training MSE': bagging_mse_train, 
         'Test MSE': bagging_mse_test, 
         'R-Squared': bagging_r2}, 
        {'Model': 'random forest of 55 depth-20  trees', 
         'Training MSE': random_forest_train_mse,
         'Test MSE': random_forest_test_mse, 
         'R-Squared': random_forest_r_squared}, 
        {'Model': 'gradient boosting machine',
         'Training MSE': gbr_mse_train, 
         'Test MSE': gbr_mse_test,
         'R-Squared': gbr_r2}
       ]
# pass column names in the columns parameter 
tree_results_df = pd.DataFrame(data)
tree_results_df.head()

"""## Support Vector Machine"""

svr_rbf = svm.SVR(kernel='rbf', gamma=0.1)
svr_rbf.fit(X_train_std, y_train)

svr_linear = svm.SVR(kernel='linear')
svr_linear.fit(X_train_std, y_train)

y_pred_train = svr_rbf.predict(X_train_std)
y_pred_test = svr_rbf.predict(X_test_std)

rbf_MSE_train = mean_squared_error(y_pred_train, y_train)
rbf_MSE_test = mean_squared_error(y_pred_test, y_test)
rbf_r_2 = svr_rbf.score(X_test_std, y_test)

y_pred_train = svr_linear.predict(X_train_std)
y_pred_test = svr_linear.predict(X_test_std)

linear_MSE_train = mean_squared_error(y_pred_train, y_train)
linear_MSE_test = mean_squared_error(y_pred_test, y_test)
linear_r_2 = svr_linear.score(X_test_std, y_test)

data = [{'Model': 'SVM rbf', 
         'Training MSE': rbf_MSE_train, 
         'Test MSE': rbf_MSE_test,
         'R-Squared': rbf_r_2}, 
        {'Model': 'SVM linear', 
         'Training MSE': linear_MSE_train, 
         'Test MSE': linear_MSE_test,
         'R-Squared': linear_r_2}]
# pass column names in the columns parameter 
svm_results_df = pd.DataFrame(data)
svm_results_df.head()

"""# Multilayer Preceptron"""

from sklearn.neural_network import MLPRegressor
reg= MLPRegressor(hidden_layer_sizes=(100,),activation="relu" ,random_state=1, max_iter=2000)
reg.fit(X_train_std, y_train)

y_pred_train=reg.predict(X_train_std)
y_pred_test=reg.predict(X_test_std)


mlp_MSE_train = mean_squared_error(y_pred_train, y_train)
mlp_MSE_test = mean_squared_error(y_pred_test, y_test)
mlp_r_2 = reg.score(X_test_std, y_test)

data = [{'Model': 'MLP', 
         'Training MSE': mlp_MSE_train, 
         'Test MSE': mlp_MSE_test,
         'R-Squared': mlp_r_2}]
# pass column names in the columns parameter 
mlp_results_df = pd.DataFrame(data)
mlp_results_df.head()

#add layers
mlp_MSE_train_list = []
mlp_MSE_test_list = []
mlp_r_2_list = []

layers = (100,)
for i in range(10):
    reg= MLPRegressor(hidden_layer_sizes=layers,activation="relu" ,random_state=1, max_iter=2000)
    reg.fit(X_train_std, y_train)

    y_pred_train=reg.predict(X_train_std)
    y_pred_test=reg.predict(X_test_std)


    mlp_MSE_train = mean_squared_error(y_pred_train, y_train)
    mlp_MSE_test = mean_squared_error(y_pred_test, y_test)
    mlp_r_2 = reg.score(X_test_std, y_test)
    
    mlp_MSE_train_list.append(mlp_MSE_train)
    mlp_MSE_test_list.append(mlp_MSE_test)
    mlp_r_2_list.append(mlp_r_2)
    
    layers = list(layers)
    layers.append(100)
    layers = tuple(layers)

plt.plot(range(1,11), mlp_MSE_test_list, color = 'red', label = 'Test')
plt.plot(range(1,11), mlp_MSE_train_list, color = 'blue', label = 'Train')
plt.xlabel('Number of Layers')
plt.ylabel('MSE')
plt.title('MSE vs Number of Layers')
plt.legend()

from sklearn.neural_network import MLPRegressor
reg= MLPRegressor(hidden_layer_sizes=(100,100,100,100),activation="relu" ,random_state=1, max_iter=2000)
reg.fit(X_train_std, y_train)

y_pred_train=reg.predict(X_train_std)
y_pred_test=reg.predict(X_test_std)


mlp_MSE_train = mean_squared_error(y_pred_train, y_train)
mlp_MSE_test = mean_squared_error(y_pred_test, y_test)
mlp_r_2 = reg.score(X_test_std, y_test)

data = [{'Model': 'MLP', 
         'Training MSE': mlp_MSE_train, 
         'Test MSE': mlp_MSE_test,
         'R-Squared': mlp_r_2}]
# pass column names in the columns parameter 
mlp_results_df = pd.DataFrame(data)
mlp_results_df.head()

"""Comparison of Models"""

df_results = pd.concat([linear_results_df, pcr_results_df, tree_results_df, svm_results_df, mlp_results_df], join="inner")
df_results

plt.barh(df_results.Model, df_results['R-Squared'], color = 'cornflowerblue')
plt.title('Model vs R Squared')

plt.barh(df_results.Model, df_results['Test MSE'], color = 'cornflowerblue')
plt.title('Model vs Test MSE')

